-----------------------------------------------------------
    Discrete planning
-----------------------------------------------------------
- state space is finite or countably infinite (integer assignable)
- no geometric models or differential equations needed
- no uncertainty (for now)

-----------------------------------------------------------
    Problem formulation
-----------------------------------------------------------
- state x
	- takes on a value from the state space X
	- goal states X_G subset X 
- action u
	- produces new state when applied to current state
		- described by state transition function f
		f(x,u) = x'
- action space U(x)
	- set of all actions applicable to state x
	- U = union{x E X} U(x)
- directed state transition graph
	- set of vertices is X
- want compact encoding of the problem

-----------------------------------------------------------
    Search for feasibility
-----------------------------------------------------------
- graph search algorithms
- explore state transition graph incrementally
	- rather than fully specified in advance (no memory)
- important for algos to be #systematic#
	- aka complete: if finite, find a solution or declare there is none in finite time
		- accomplished by keeping track of already visited states and avoiding redundant visits
	- if infinite, find a solution in finite time if it exists, or search forever if there is none
		- accomplished by exploring every reachable vertex as search iterations -> infinite
- pretty much all the same algorithm (generalization of BFS)
	- only difference in ordering of priority queue and augmented info for each node
- unified search algo
	- incrementally construct search graph G(V,E)
		- G is subgraph of state transition graph and is the one explored so far
	1. initialize
		- E empty
		- V contains some starting nodes
			- V = {x_I} for forward search
			- V = {x_G} for backward search
			- V = {x_I, x_G} for bidirectional
	2. select vertex
		- choose n_curr in V for expansion
		- usually done by maintaining priority queue
		- x_curr is the state associated with n_curr
	3. apply an action
		- get new state x_new
		- x_new = f(x, u) for some u in U(x) for forward search
		- x = f(x_new, u) for some u in U(x_new) for backward
	4. insert directed edge into graph
		- if algo-specific tests are passed
		- if x_new not yet in V, insert into V
	5. check for solution
		- whether path exists from x_I to x_G
		- trivial if there's only a single search tree
	6. return to step 2

-----------------------------------------------------------
    Search for optimality
-----------------------------------------------------------
- uses most of the same concepts
	- new #cost functional# L
- choose to min (arbitrary selection and symmetric with max)
- first consider simpler fixed length plans
	- P_K is a K-step plan
	- P_K = (u_1, u_2, ..., u_K) K actions
	- (x_1, x_2, ..., x_K+1) derived from x_1 and f given P_K
	- let L = K + 1
	- L(P_K) = sum{k=1..K} l(x_k, u_k) + l_F(x_F)
		- l_F(x_F) = 0 if x_F in X_G else inf
		- l_F is unnecessary if L is only applied to plans that reach goal
			- but this makes notation more cumbersome to restrict the action space
			- having l_F allows extending L's domain to all plans
			- trick to convert feasibile hard constraint into optimization
	- to get feasible solution restricted to K steps, let l(x,u) := 0
	- to get min step solution <= K steps, let l(x,u) := 1
	- can let l_F(x) have != 0 values when x in X_G to represent different quality of goals
- principle of optimality: portions of an optimal plan are optimal themselves
	- if a better portion could be substituted, a better plan is found which is a contradiction
	- leads directly to #value iteration#
		- iteratively compute cost to come or cost to go
		- backward value iteration
			- uses cost to go (to x_G)
			- fixed x_G
			- let G*_k be the accumulated cost from stage k to F under execution of the optimal plan
				G*_k(x_k) = min{u_k, ... u_K}( sum{i=k..K} l(x_i, u_i) + l_F(x_F))
			- when k = F, G*_F(x_F) = l_F(x_F)
			- compute G*_k from G*_k+1
				- factor min{u_k} and l(x_k, u_k) out
				G*_k(x_k) = min{u_k}(l(x_k, u_k) + G*_k+1(x_k+1))
			- in each iteration some states will receive inf value due to unreachability
			- get G*_1 in O(K|X||U|) time
				- optimal cost to go from any initial state
		- forward value iteration
			- uses cost to come (from x_I)
			- fixed x_I
			- finds optimal plans from one initial to all goal states

-----------------------------------------------------------
    Unspecified lengths
-----------------------------------------------------------
- replace specification of K with special action u_T
	- L(P_K) = sum{k=1..K} l(x_k, u_k) + l_F(x_F) looks like the same
		- K is not pretermined so the domain of L is much larger than before
	- #termination action# u_T
		- applying u_T at x_k repeats the action forever
			- for all i >= k, u_i = u_T, x_i = x_k, l(x_i, u_T) = 0
	- plan with (u1, u2) is equivalent with (u1, u2, u_T, u_T, u_T)
	- allows optimization over all plans of length <= K
		- instead of fixed K

-----------------------------------------------------------
    Dijkstra
-----------------------------------------------------------
- similar to forward value iteration
- focuses on interesting (non-stationary value) nodes
	- good for some problems
	- not suitable for problems where this complicates the model such that computing it becomes intractable

-----------------------------------------------------------
    Logic Representation of Discrete Planning
-----------------------------------------------------------
- need implicit encodings of giant problems (10^100 states)
- can use logic to represent certain kinds of problems compactly
	- output is also human-friendly
	- historically popular
- difficult to generalize (especially compared to state-space)
	- continuous spaces
	- unpredictability
	- sensing uncertainty
	- multiple decision makers
